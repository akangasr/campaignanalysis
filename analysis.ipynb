{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = './training_data.csv'\n",
    "test_data_file = './test_data.csv'\n",
    "training_data = pd.read_csv(training_data_file, sep=';')\n",
    "test_data = pd.read_csv(test_data_file, sep=';', nrows=2190030)  # EOF inside string starting at row 2190031 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data):\n",
    "    df = data.copy()\n",
    "    # create population-level model\n",
    "    df = df.drop('id', axis=1)\n",
    "    # extract possible predictive features from timestamp and make last start relative\n",
    "    df['timestamp'] = df['timestamp'].astype('datetime64[ns]')\n",
    "    df['lastStart'] = df['lastStart'].astype('datetime64[ns]')\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek.astype('category')\n",
    "    df['logLastStartH'] = np.log(((df['timestamp'] - df['lastStart']).dt.total_seconds() + 1) / 3600)\n",
    "    df = df.drop('timestamp', axis=1)\n",
    "    df = df.drop('lastStart', axis=1)\n",
    "    # drop categorical columns that seem to have too many distinct values to be useful\n",
    "    df = df.drop('sourceGameId', axis=1)\n",
    "    df = df.drop('deviceType', axis=1)\n",
    "    # drop categorical colums where distribution of labels doesn't match well between training and test data\n",
    "    df = df.drop('campaignId', axis=1)\n",
    "    df = df.drop('softwareVersion', axis=1)\n",
    "    # logarithmic transforms\n",
    "    df['logStartCount'] = np.log(df['startCount'] + 1)\n",
    "    df = df.drop('startCount', axis=1)\n",
    "    df['logViewCount'] = np.log(df['viewCount'] + 1)\n",
    "    df = df.drop('viewCount', axis=1)\n",
    "    df['logClickCount'] = np.log(df['clickCount'] + 1)\n",
    "    df = df.drop('clickCount', axis=1)\n",
    "    df['logInstallCount'] = np.log(df['installCount'] + 1)\n",
    "    df = df.drop('installCount', axis=1)\n",
    "    df['logStartCount1d'] = np.log(df['startCount1d'] + 1)\n",
    "    df = df.drop('startCount1d', axis=1)\n",
    "    df['logStartCount7d'] = np.log(df['startCount7d'] + 1)\n",
    "    df = df.drop('startCount7d', axis=1)\n",
    "    # set types\n",
    "    df['platform'] = data['platform'].astype('category')\n",
    "    df['country'] = data['country'].astype('category')\n",
    "    df['connectionType'] = data['connectionType'].astype('category')\n",
    "    # drop features with very little apparent predictive power\n",
    "    df = df.drop('dayofweek', axis=1)\n",
    "    df = df.drop('platform', axis=1)\n",
    "    df = df.drop('connectionType', axis=1)\n",
    "    # drop nan\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "tr = process(training_data)\n",
    "te = process(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check stats for continuous variables\")\n",
    "tr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check number of categories for categorical variables\")\n",
    "for key in tr.select_dtypes(['category']).columns:\n",
    "    print(key, len(tr[key].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Compare marginal distributions of training and test data\")\n",
    "for key in te.keys():\n",
    "    print(key)\n",
    "    if key in ['campaignId', 'softwareVersion', 'country']:\n",
    "        tr_set = set(tr[key].unique())\n",
    "        te_set = set(te[key].unique())\n",
    "        print(\"- Training {} keys of which {} also in test set\".format(\n",
    "            len(tr_set), len(tr_set.intersection(te_set))))\n",
    "        print(\"- Test {} keys of which {} not in training set\".format(\n",
    "            len(te_set), len(te_set.difference(tr_set))))\n",
    "    else:\n",
    "        tr[key].hist()\n",
    "        plt.show()\n",
    "        te[key].hist()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_encode(data1, data2):\n",
    "    # encode categorical variables as int\n",
    "    # make sure training and test data use same encoding schema\n",
    "    joint_data = pd.concat([data1, data2], axis=0)\n",
    "    joint_data['country'] = joint_data['country'].astype('category')\n",
    "    df1 = data1.copy()\n",
    "    df2 = data2.copy()\n",
    "    cat_columns = joint_data.select_dtypes(['category']).columns\n",
    "    for cat_column in cat_columns:\n",
    "        df1[cat_column] = joint_data[cat_column][:len(data1)].cat.codes\n",
    "        df2[cat_column] = joint_data[cat_column][len(data1):].cat.codes\n",
    "    return df1, df2\n",
    "\n",
    "tre, tee = joint_encode(tr, te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"Naive Bayes predictor as probability of install was requested\"\"\"\n",
    "    def __init__(self, data, only_features=[], not_features=[], train_frac=0.8):\n",
    "        # set weights as dataset is biased\n",
    "        c0, c1 = data.install.value_counts().tolist()\n",
    "        self.w0 = c1 / (c1 + c0)\n",
    "        self.w1 = c0 / (c1 + c0)\n",
    "        # split\n",
    "        self.train = data.sample(frac=train_frac, random_state=200)\n",
    "        self.test = data.drop(self.train.index)\n",
    "        # features to use\n",
    "        self.cat_features = tr.select_dtypes(['category']).columns\n",
    "        self.con_features = tr.select_dtypes(['float64']).columns\n",
    "        if only_features:\n",
    "            self.cat_features = [c for c in self.cat_features if c in only_features]\n",
    "            self.con_features = [c for c in self.con_features if c in only_features]\n",
    "        if not_features:\n",
    "            self.cat_features = [c for c in self.cat_features if c not in not_features]\n",
    "            self.con_features = [c for c in self.con_features if c not in not_features]\n",
    "        # fit models\n",
    "        self._fit_categorical_model()\n",
    "        self._fit_continuous_model()\n",
    "        \n",
    "    def _fit_categorical_model(self):\n",
    "        if not any(self.cat_features):\n",
    "            self.catm = None\n",
    "            return\n",
    "        self.catm = CategoricalNB()\n",
    "        self.catm.fit(self.train[self.cat_features], self.train['install'])\n",
    "\n",
    "    def _fit_continuous_model(self):\n",
    "        if not any(self.con_features):\n",
    "            self.conm = None\n",
    "            return\n",
    "        self.conm = GaussianNB()\n",
    "        self.conm.fit(self.train[self.con_features], self.train['install'])\n",
    "\n",
    "    def pred(self, data):\n",
    "        if self.catm and self.conm:\n",
    "            cat_pred = self.catm.predict_proba(data[self.cat_features])[:,1]\n",
    "            con_pred = self.conm.predict_proba(data[self.con_features])[:,1]\n",
    "            return (cat_pred + con_pred) / 2  # roughly equally accurate\n",
    "        if self.catm:\n",
    "            return self.catm.predict_proba(data[self.cat_features])[:,1]\n",
    "        if self.conm:\n",
    "            return self.conm.predict_proba(data[self.con_features])[:,1]\n",
    "        assert False\n",
    "        \n",
    "    def pred_eval(self, data):\n",
    "        pred = self.pred(data)\n",
    "        weights = data['install'] * self.w1 - (data['install'] - 1) * self.w0\n",
    "        print(\"- Loss {:.3f}\".format(log_loss(data['install'], pred, sample_weight=weights)))\n",
    "\n",
    "        \n",
    "all_features = tr.select_dtypes(['category', 'float64']).columns\n",
    "print(\"All features\")\n",
    "p = Predictor(tre)\n",
    "p.pred_eval(tre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Individual feature predictivity\")\n",
    "for feature in all_features:\n",
    "    print(feature)\n",
    "    p = Predictor(tre, only_features=[feature])\n",
    "    p.pred_eval(tre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Effect of removing individual feature\")\n",
    "for feature in all_features:\n",
    "    print(feature)\n",
    "    p = Predictor(tre, not_features=[feature])\n",
    "    p.pred_eval(tre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Predictor(tre)\n",
    "preds = p.pred(tee)\n",
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({'prob_install': preds}, index=te.index.values.tolist())\n",
    "out_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('test_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
